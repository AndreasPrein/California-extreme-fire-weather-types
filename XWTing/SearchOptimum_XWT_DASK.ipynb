{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'File name: ExtremeEvent-WeatherTyping.py\\n    Author: Andreas Prein\\n    E-mail: prein@ucar.edu\\n    Date created: 16.04.2018\\n    Date last modified: 16.04.2018\\n\\n    ############################################################## \\n    Purpos:\\n    Classifies weather patterns that cause precipitation extremes\\n\\n    1) read in shape file for area under cosideration\\n\\n    2) read in precipitation data from PRISM\\n\\n    3) identify the N-days that had highest rainfall records\\n\\n    4) read in ERA-Interim data for these days\\n\\n    5) remove the 30-year mooving average from time series and\\n       normalize the variables\\n\\n    5) run clustering algorithm on extreme WT patterns\\n\\n    6) search for the extreme WT centroids in the full record\\n\\n\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "'''File name: ExtremeEvent-WeatherTyping.py\n",
    "    Author: Andreas Prein\n",
    "    E-mail: prein@ucar.edu\n",
    "    Date created: 16.04.2018\n",
    "    Date last modified: 16.04.2018\n",
    "\n",
    "    ############################################################## \n",
    "    Purpos:\n",
    "    Classifies weather patterns that cause precipitation extremes\n",
    "\n",
    "    1) read in shape file for area under cosideration\n",
    "\n",
    "    2) read in precipitation data from PRISM\n",
    "\n",
    "    3) identify the N-days that had highest rainfall records\n",
    "\n",
    "    4) read in ERA-Interim data for these days\n",
    "\n",
    "    5) remove the 30-year mooving average from time series and\n",
    "       normalize the variables\n",
    "\n",
    "    5) run clustering algorithm on extreme WT patterns\n",
    "\n",
    "    6) search for the extreme WT centroids in the full record\n",
    "\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing TKAgg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/glade/u/apps/dav/opt/python/3.7.5/gnu/8.3.0/pkg-library/20200417/lib/python3.7/site-packages/ipykernel_launcher.py:6: MatplotlibDeprecationWarning: The 'warn' parameter of use() is deprecated since Matplotlib 3.1 and will be removed in 3.3.  If any parameter follows 'warn', they should be pass as keyword, not positionally.\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using: TkAgg\n"
     ]
    }
   ],
   "source": [
    "import matplotlib\n",
    "gui_env = ['TKAgg','GTKAgg','Qt4Agg','WXAgg']\n",
    "for gui in gui_env:\n",
    "    try:\n",
    "        print( \"testing\", gui)\n",
    "        matplotlib.use(gui,warn=False, force=True)\n",
    "        from matplotlib import pyplot as plt\n",
    "        break\n",
    "    except:\n",
    "        continue\n",
    "print( \"Using:\",matplotlib.get_backend())\n",
    "\n",
    "\n",
    "from dateutil import rrule\n",
    "import datetime\n",
    "import glob\n",
    "from netCDF4 import Dataset\n",
    "import sys, traceback\n",
    "import dateutil.parser as dparser\n",
    "import string\n",
    "from pdb import set_trace as stop\n",
    "import numpy as np\n",
    "import numpy.ma as ma\n",
    "import os\n",
    "from mpl_toolkits import basemap\n",
    "import pickle\n",
    "import subprocess\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as colors\n",
    "import matplotlib as mpl\n",
    "import pylab as plt\n",
    "import random\n",
    "import scipy.ndimage as ndimage\n",
    "import scipy\n",
    "import shapefile\n",
    "import matplotlib.path as mplPath\n",
    "from matplotlib.patches import Polygon as Polygon2\n",
    "# Cluster specific modules\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "from scipy.cluster.hierarchy import cophenet\n",
    "from scipy.spatial.distance import pdist\n",
    "from scipy.cluster.hierarchy import fcluster\n",
    "from scipy.cluster.vq import kmeans2,vq, whiten\n",
    "from scipy.ndimage import gaussian_filter\n",
    "import seaborn as sns\n",
    "# import metpy.calc as mpcalc\n",
    "import shapefile as shp\n",
    "import sys\n",
    "from itertools import combinations \n",
    "\n",
    "from Functions_Extreme_WTs import XWT\n",
    "from Functions_Extreme_WTs import MRR, MRD, perkins_skill\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import time\n",
    "import dask\n",
    "from dask.distributed import Client, progress\n",
    "from dask import delayed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of var. combinations 6017\n",
      "Number of arguments: 3 arguments.\n",
      "---- Process Region Central_Coast ----\n",
      "    Months: 1-2-3-4-5-6-7-8-9-10-11-12\n"
     ]
    }
   ],
   "source": [
    "# ###################################################\n",
    "# This information comes from the setup file\n",
    "\n",
    "from XWTs_search_ERA5 import rgdTime, iMonths, sPlotDir, sDataDir, sSubregionPR, rgsWTvars, VarsFullName,rgsWTfolders, rgrNrOfExtremes, WT_Domains, DomDegreeAdd, Annual_Cycle, SpatialSmoothing, Metrics, Dimensions, DW_Regions, FireObs\n",
    "# from XWTs_search_ERA5_preselect import rgdTime, iMonths, sPlotDir, sDataDir, sSubregionPR, rgsWTvars, VarsFullName,rgsWTfolders, rgrNrOfExtremes, WT_Domains, DomDegreeAdd, Annual_Cycle, SpatialSmoothing, Metrics, Dimensions, DW_Regions, FireObs\n",
    "\n",
    "\n",
    "rgsWTfolders=[rgsWTfolders[0]+str(rgsWTvars[va])+'/' for va in range(len(rgsWTvars))]\n",
    "\n",
    "# create all possible combinations of variables\n",
    "iVariables=range(len(VarsFullName))\n",
    "Combinations1=np.array(list(combinations(iVariables, 1)))\n",
    "Combinations2=np.squeeze(np.array(list(combinations(iVariables, 2))))\n",
    "Combinations3=np.squeeze(np.array(list(combinations(iVariables, 3))))\n",
    "Combinations4=np.squeeze(np.array(list(combinations(iVariables, 4))))\n",
    "Combinations=list(Combinations1)+list(Combinations2)+list(Combinations3) #+list(Combinations4)\n",
    "print('Number of var. combinations '+str(len(Combinations)))\n",
    "\n",
    "print('Number of arguments:', len(sys.argv), 'arguments.')\n",
    "iRegion=1 #int(sys.argv[1])\n",
    "sRegion=DW_Regions[iRegion]\n",
    "sSubregionPR=sSubregionPR+sRegion\n",
    "print('---- Process Region '+sRegion+' ----')\n",
    "\n",
    "# create nessesary directories\n",
    "if not os.path.exists(sDataDir):\n",
    "    os.makedirs(sDataDir)\n",
    "if not os.path.exists(sPlotDir):\n",
    "    os.makedirs(sPlotDir)\n",
    "sRegion=sRegion.replace('/','-')\n",
    "\n",
    "ss='-'\n",
    "sMonths=ss.join([str(iMonths[ii]) for ii in range(len(iMonths))])\n",
    "print('    Months: '+sMonths)\n",
    "\n",
    "# ###################################################\n",
    "# use setup to generate data\n",
    "rgiYears=np.unique(rgdTime.year)\n",
    "rgdTimeMM = pd.date_range(datetime.datetime(rgdTime.year[0], rgdTime.month[0], rgdTime.day[0],0), end=datetime.datetime(rgdTime.year[-1], rgdTime.month[-1], rgdTime.day[-1],0), freq='m')\n",
    "AllMonths=rgdTime\n",
    "YYYY_stamp=str(rgdTime.year[0])+'-'+str(rgdTime.year[-1])\n",
    "rgiSeasonWT=np.isin(rgdTime.month, iMonths)\n",
    "rgdTime=rgdTime[rgiSeasonWT]\n",
    "\n",
    "SPLIT=np.where(rgdTime.year < rgiYears[int(np.round(len(rgiYears)/2))])[0][-1]\n",
    "    \n",
    "SkillScores_All=np.zeros((len(Combinations),len(rgrNrOfExtremes),len(WT_Domains),len(Annual_Cycle),len(SpatialSmoothing),2,len(Metrics))); SkillScores_All[:]=np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Read the daily Fire data\n",
      "    Read the ERA-Interim data\n",
      "        Read ERA-5 year: 2001\n",
      "        Read ERA-5 year: 2002\n",
      "        Read ERA-5 year: 2003\n",
      "        Read ERA-5 year: 2004\n",
      "        Read ERA-5 year: 2005\n",
      "        Read ERA-5 year: 2006\n",
      "        Read ERA-5 year: 2007\n",
      "        Read ERA-5 year: 2008\n",
      "        Read ERA-5 year: 2009\n",
      "        Read ERA-5 year: 2010\n",
      "        Read ERA-5 year: 2011\n",
      "        Read ERA-5 year: 2012\n",
      "        Read ERA-5 year: 2013\n",
      "        Read ERA-5 year: 2014\n",
      "        Read ERA-5 year: 2015\n",
      "        Read ERA-5 year: 2016\n",
      "        Read ERA-5 year: 2017\n",
      "        Read ERA-5 year: 2018\n",
      "        Read ERA-5 year: 2019\n",
      "    Read the ERA-Interim data specific for the region\n",
      "    ------\n",
      "    Domain S\n",
      "    Split Sample Nr. 1\n",
      "        4 EXTREMES\n",
      "            Loop over variable permutations\n",
      "> <ipython-input-4-3874470b6e79>(147)<module>()\n",
      "-> for va1 in range(len(Combinations)):\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "(Pdb)  exit\n"
     ]
    },
    {
     "ename": "BdbQuit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mBdbQuit\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-3874470b6e79>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    145\u001b[0m                         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0;34m'            Loop over variable permutations'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m                         \u001b[0mstop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m                         \u001b[0;32mfor\u001b[0m \u001b[0mva1\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCombinations\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    148\u001b[0m                             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'    process var '\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mva1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m' of '\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCombinations\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m                             XWT_output=XWT(DailyVarsTrain[:,:,:,Combinations[va1]],\n",
      "\u001b[0;32m<ipython-input-4-3874470b6e79>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    145\u001b[0m                         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0;34m'            Loop over variable permutations'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m                         \u001b[0mstop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m                         \u001b[0;32mfor\u001b[0m \u001b[0mva1\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCombinations\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    148\u001b[0m                             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'    process var '\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mva1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m' of '\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCombinations\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m                             XWT_output=XWT(DailyVarsTrain[:,:,:,Combinations[va1]],\n",
      "\u001b[0;32m/glade/u/apps/dav/opt/python/3.7.5/gnu/8.3.0/lib/python3.7/bdb.py\u001b[0m in \u001b[0;36mtrace_dispatch\u001b[0;34m(self, frame, event, arg)\u001b[0m\n\u001b[1;32m     86\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;31m# None\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mevent\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'line'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mevent\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'call'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/glade/u/apps/dav/opt/python/3.7.5/gnu/8.3.0/lib/python3.7/bdb.py\u001b[0m in \u001b[0;36mdispatch_line\u001b[0;34m(self, frame)\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_here\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbreak_here\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muser_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquitting\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mraise\u001b[0m \u001b[0mBdbQuit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrace_dispatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mBdbQuit\u001b[0m: "
     ]
    }
   ],
   "source": [
    "### CHECK IF DATA IS ALREADY PROCESSED ###\n",
    "SaveStats=sDataDir+sRegion+'_'+YYYY_stamp+'-'+sMonths+'_'+FireObs+'_ERA5.npz'\n",
    "if os.path.isfile(SaveStats) == 0:\n",
    "    # ###################################################\n",
    "    print('    Read the daily Fire data')\n",
    "    \n",
    "    \n",
    "    ncid=Dataset('/glade/campaign/mmm/c3we/prein/observations/Fire-observations/Fire_GWIS/data_20020702/GriddedData/MODIS_BA_GLOBAL_1_2016_01_gridded.nc', mode='r') # open the netcdf file\n",
    "    rgrLatPR=np.squeeze(ncid.variables['rlat'][:])\n",
    "    rgrLonPR=np.squeeze(ncid.variables['rlon'][:])\n",
    "    ncid.close()\n",
    "    rgrGridCells=[(rgrLonPR.ravel()[ii],rgrLatPR.ravel()[ii]) for ii in range(len(rgrLonPR.ravel()))]\n",
    "    rgrSRactP=np.zeros((rgrLonPR.shape[0]*rgrLonPR.shape[1]))\n",
    "    sf = shp.Reader(sSubregionPR)\n",
    "    from Functions_Extreme_WTs import read_shapefile\n",
    "    df = read_shapefile(sf)\n",
    "    for sf in range(df.shape[0]):\n",
    "        ctr = df['coords'][sf]\n",
    "        if len(ctr) > 10000:\n",
    "            ctr=np.array(ctr)[::100,:] # carsen the shapefile accuracy\n",
    "        else:\n",
    "            ctr=np.array(ctr)\n",
    "        grPRregion=mplPath.Path(ctr)\n",
    "        TMP=np.array(grPRregion.contains_points(rgrGridCells))\n",
    "        rgrSRactP[TMP == 1]=1\n",
    "\n",
    "    rgrSRactP=np.reshape(rgrSRactP, (rgrLatPR.shape[0], rgrLatPR.shape[1]))\n",
    "    rgiSrPR=np.array(np.where(rgrSRactP == True))\n",
    "    iLatMaxP=rgiSrPR[0,:].max()+1\n",
    "    iLatMinP=rgiSrPR[0,:].min()\n",
    "    iLonMaxP=rgiSrPR[1,:].max()+1\n",
    "    iLonMinP=rgiSrPR[1,:].min()\n",
    "    rgrPRdata=np.zeros((sum(rgiSeasonWT),iLatMaxP-iLatMinP,iLonMaxP-iLonMinP))\n",
    "        \n",
    "    if FireObs == 'MODIS':\n",
    "        for mo in range(len(rgdTimeMM)):\n",
    "            rgiDD=np.where(((rgdTimeMM.year[mo] == rgdTime.year) & (rgdTime.month == rgdTimeMM.month[mo]) & np.isin(rgdTimeMM.month[mo], iMonths)))[0]\n",
    "            if len(rgiDD) != 0:\n",
    "                ncid=Dataset('/glade/campaign/mmm/c3we/prein/observations/Fire-observations/Fire_GWIS/data_20020702/GriddedData/MODIS_BA_GLOBAL_1_'+str(rgdTimeMM.year[mo])+'_'+str(\"%02d\" % rgdTimeMM.month[mo])+'_gridded.nc', mode='r')\n",
    "                rgrPRdata[rgiDD,:,:]=np.squeeze(ncid.variables['BurnedArea'][:,iLatMinP:iLatMaxP,iLonMinP:iLonMaxP])\n",
    "                ncid.close()\n",
    "        \n",
    "        rgrPRdata[rgrPRdata<0] = np.nan\n",
    "        rgiSRgridcells=rgrSRactP[iLatMinP:iLatMaxP,iLonMinP:iLonMaxP].astype('int')\n",
    "        rgrPRdata=np.nanmean(rgrPRdata[:,(rgiSRgridcells==1)], axis=(1))\n",
    "\n",
    "    elif FireObs == 'Parks':\n",
    "        SheanDataDir='/glade/campaign/mmm/c3we/prein/observations/Fire-observations/Sean_Parks/SubRegion_DailyBurnedArea/'\n",
    "        FILE=SheanDataDir+DW_Regions[iRegion]+'.txt'\n",
    "        rgrPRrecords=np.array(pd.read_csv(FILE, delim_whitespace=True, header = None))[:,0]\n",
    "        \n",
    "        # rgiExtrTrain=ExtremeDays(rgrPRrecords,iNrOfExtremes,MinDistDD)\n",
    "        # rgiExtremeDays=rgdTime[rgiExtrTrain]\n",
    "        rgrPRdata=np.copy(rgrPRrecords)\n",
    "        rgrPRdata[rgrPRdata<0] = np.nan\n",
    "\n",
    "    print( '    Read the ERA-Interim data')\n",
    "    # We read in ERA-Interim data for the largest region and cut it to fit smaller regions\n",
    "    DomDelta=np.max(DomDegreeAdd)\n",
    "    Wlon=ctr[:,0].min()\n",
    "    Elon=ctr[:,0].max()\n",
    "    Nlat=ctr[:,1].max()\n",
    "    Slat=ctr[:,1].min()\n",
    "    DomainWT=np.array([[Elon+DomDelta,Slat-DomDelta],\n",
    "                       [Wlon-DomDelta,Slat-DomDelta],\n",
    "                       [Wlon-DomDelta,Nlat+DomDelta],\n",
    "                       [Elon+DomDelta,Nlat+DomDelta],\n",
    "                       [Elon+DomDelta,Slat-DomDelta]])\n",
    "    grWTregion=mplPath.Path(DomainWT)\n",
    "\n",
    "    # ###################################################\n",
    "    #         Read the ERA-Interim grid and data\n",
    "    from Functions_Extreme_WTs import ReadERA5\n",
    "    DailyVarsLargeDom=ReadERA5(grWTregion,      # shapefile with WTing region\n",
    "                       rgdTime,         # time period for WTing\n",
    "                       iMonths,         # list of months that should be considered\n",
    "                       rgsWTfolders,    # directories containing WT files\n",
    "                       rgsWTvars)       # netcdf variable names of WT variables\n",
    "\n",
    "    # for CAPE and CIN replace NaN with 0\n",
    "    VARSact=['CAPE','CIN']\n",
    "    for vv in range(len(VARSact)):\n",
    "        try:\n",
    "            IND=VarsFullName.index(VARSact[vv])\n",
    "            TMP=DailyVarsLargeDom[0][:,:,:,IND]; TMP[np.isnan(TMP)]=0\n",
    "            DailyVarsLargeDom[0][:,:,:,IND]=TMP\n",
    "        except:\n",
    "            pass\n",
    "    # ###################################################\n",
    "    print('    Read the ERA-Interim data specific for the region')\n",
    "    \n",
    "    for re in range(len(WT_Domains)):\n",
    "        print( '    ------')\n",
    "        print( '    Domain '+WT_Domains[re])\n",
    "        DeltaX=np.max(DomDegreeAdd)-DomDegreeAdd[re]\n",
    "        if DeltaX != 0:\n",
    "            DomainWT=np.array([[Elon+DomDegreeAdd[re],Slat-DomDegreeAdd[re]],\n",
    "                       [Wlon-DomDegreeAdd[re],Slat-DomDegreeAdd[re]],\n",
    "                       [Wlon-DomDegreeAdd[re],Nlat+DomDegreeAdd[re]],\n",
    "                       [Elon+DomDegreeAdd[re],Nlat+DomDegreeAdd[re]],\n",
    "                       [Elon+DomDegreeAdd[re],Slat-DomDegreeAdd[re]]])\n",
    "\n",
    "            grWTregion=mplPath.Path(DomainWT)\n",
    "            rgrGridCells=[(DailyVarsLargeDom[1].ravel()[ii],DailyVarsLargeDom[2].ravel()[ii]) for ii in range(len(DailyVarsLargeDom[1].ravel()))]\n",
    "            rgrSRact=np.array(grWTregion.contains_points(rgrGridCells)); rgrSRact=np.reshape(rgrSRact, (DailyVarsLargeDom[1].shape[0], DailyVarsLargeDom[1].shape[1]))\n",
    "            rgiSrWT=np.array(np.where(rgrSRact == True))\n",
    "            iLatMax=rgiSrWT[0,:].max()\n",
    "            iLatMin=rgiSrWT[0,:].min()\n",
    "            iLonMax=rgiSrWT[1,:].max()\n",
    "            iLonMin=rgiSrWT[1,:].min()\n",
    "\n",
    "            DailyVars=DailyVarsLargeDom[0][:,iLatMin:iLatMax,iLonMin:iLonMax,:]\n",
    "        else:\n",
    "            DailyVars=DailyVarsLargeDom[0]\n",
    "        # perform split sample statistic\n",
    "        for ss in range(2):\n",
    "            print('    Split Sample Nr. '+str(ss+1))\n",
    "            if ss == 0:\n",
    "                DailyVarsTrain=DailyVars[:SPLIT,:]\n",
    "                DailyVarsEval=DailyVars[-SPLIT:,:]\n",
    "                Ptrain=rgrPRdata[:SPLIT]\n",
    "                Peval=rgrPRdata[-SPLIT:]\n",
    "                TimeTrain=rgdTime[:SPLIT]\n",
    "                TimeEval=rgdTime[-SPLIT:]\n",
    "            else:\n",
    "                DailyVarsTrain=DailyVars[-SPLIT:,:]\n",
    "                DailyVarsEval=DailyVars[:SPLIT,:]\n",
    "                Ptrain=rgrPRdata[-SPLIT:]\n",
    "                Peval=rgrPRdata[:SPLIT]\n",
    "                TimeTrain=rgdTime[-SPLIT:]\n",
    "                TimeEval=rgdTime[:SPLIT]\n",
    "\n",
    "            for ne in range(len(rgrNrOfExtremes)):\n",
    "                DailyVarsAct=np.copy(DailyVarsTrain)\n",
    "                print( '        '+str(rgrNrOfExtremes[ne])+' EXTREMES')\n",
    "                iNrOfExtremes=rgrNrOfExtremes[ne]   # we consider the N highest rainfall extremes\n",
    "                rgiSRgridcells=rgrSRactP[iLatMinP:iLatMaxP,iLonMinP:iLonMaxP].astype('int')\n",
    "                rgrPRrecords=Ptrain\n",
    "                rgrPReval=Peval\n",
    "    \n",
    "                # Test effect of spatial smoothing\n",
    "                for sm in range(len(SpatialSmoothing)):\n",
    "                    # annual cycle treatment\n",
    "                    for ac in range(len(Annual_Cycle)):\n",
    "                        print( '            Loop over variable permutations')\n",
    "                        stop()\n",
    "                        for va1 in range(len(Combinations)):\n",
    "                            print('    process var '+str(va1)+' of '+str(len(Combinations)))\n",
    "                            XWT_output=XWT(DailyVarsTrain[:,:,:,Combinations[va1]],\n",
    "                                           DailyVarsEval[:,:,:,Combinations[va1]],\n",
    "                                           rgrPRrecords,\n",
    "                                           rgrPReval,\n",
    "                                           TimeTrain,\n",
    "                                           TimeEval,\n",
    "                                           rgrNrOfExtremes[ne],\n",
    "                                           SpatialSmoothing[sm],\n",
    "                                           ClusterMeth='hdbscan')\n",
    "                            if XWT_output != None:\n",
    "                                SkillScores_All[va1, ne, re, ac, sm, ss, Metrics.index('PSS')]=XWT_output['grPSS'] # Perkins Skill Score\n",
    "                                SkillScores_All[va1, ne, re, ac, sm, ss, Metrics.index('MRD')]=XWT_output['grMRD'] # Mean relative difference\n",
    "                                SkillScores_All[va1, ne, re, ac, sm, ss, Metrics.index('MRR')]=XWT_output['grMRR'] # Mean Rank Ratio\n",
    "                                SkillScores_All[va1, ne, re, ac, sm, ss, Metrics.index('APR')]=XWT_output['APR'] # Average precision-recall score\n",
    "                                SkillScores_All[va1, ne, re, ac, sm, ss, Metrics.index('PEX')]=XWT_output['PEX'] # Percent of points excluded for ED larger than the 75 percentile\n",
    "                                SkillScores_All[va1, ne, re, ac, sm, ss, Metrics.index('AUC')]=XWT_output['AUC'] # Area under the ROC curve\n",
    "                            \n",
    "                print( ' ')\n",
    "    np.savez(SaveStats,\n",
    "             SkillScores_All=SkillScores_All, \n",
    "             Combinations=Combinations, \n",
    "             rgsWTvars=VarsFullName,\n",
    "             rgrNrOfExtremes=rgrNrOfExtremes,\n",
    "             WT_Domains=WT_Domains,\n",
    "             Annual_Cycle=Annual_Cycle,\n",
    "             SpatialSmoothing=SpatialSmoothing,\n",
    "             Metrics=Metrics,\n",
    "             Dimensions=Dimensions)\n",
    "else:\n",
    "    print('    Load: '+SaveStats)\n",
    "    DATA=np.load(SaveStats)\n",
    "    SkillScores_All=DATA['SkillScores_All']\n",
    "    Combinations=DATA['Combinations']\n",
    "    VarsFullName=DATA['rgsWTvars']\n",
    "    rgrNrOfExtremes=DATA['rgrNrOfExtremes']\n",
    "    WT_Domains=DATA['WT_Domains']\n",
    "    Annual_Cycle=DATA['Annual_Cycle']\n",
    "    SpatialSmoothing=DATA['SpatialSmoothing']\n",
    "    Metrics=DATA['Metrics']\n",
    "    Dimensions=DATA['Dimensions']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'ncar_jobqueue'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-d6ab85dae4c9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mncar_jobqueue\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mNCARCluster\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistributed\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mClient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'ncar_jobqueue'"
     ]
    }
   ],
   "source": [
    "from ncar_jobqueue import NCARCluster\n",
    "from dask.distributed import Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Casper\n",
    "NUMNODES = 3\n",
    "num_processes = 9\n",
    "num_threads = 18\n",
    "MEM = \"200GB\"\n",
    "\n",
    "cluster = NCARCluster(cores=num_threads, \n",
    "                      processes=num_processes, \n",
    "                      memory=MEM, walltime=\"02:00:00\", \n",
    "                      project=\"p66770001\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster.scale(NUMNODES * num_processes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = Client(cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table style=\"border: 2px solid white;\">\n",
       "<tr>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Client</h3>\n",
       "<ul style=\"text-align: left; list-style: none; margin: 0; padding: 0;\">\n",
       "  <li><b>Scheduler: </b>tcp://127.0.0.1:38409</li>\n",
       "  <li><b>Dashboard: </b><a href='http://127.0.0.1:45783/status' target='_blank'>http://127.0.0.1:45783/status</a>\n",
       "</ul>\n",
       "</td>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Cluster</h3>\n",
       "<ul style=\"text-align: left; list-style:none; margin: 0; padding: 0;\">\n",
       "  <li><b>Workers: </b>10</li>\n",
       "  <li><b>Cores: </b>100</li>\n",
       "  <li><b>Memory: </b>322.12 GB</li>\n",
       "</ul>\n",
       "</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<Client: 'tcp://127.0.0.1:38409' processes=10 threads=100, memory=322.12 GB>"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# client = Client(threads_per_worker=1, n_workers=4)\n",
    "\n",
    "from distributed import LocalCluster, Client\n",
    "client = Client(LocalCluster(processes=True, n_workers=10, threads_per_worker=10))\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    process var 0 of 6017\n",
      "    process var 1 of 6017\n",
      "    process var 2 of 6017\n",
      "CPU times: user 4.71 s, sys: 2.01 s, total: 6.72 s\n",
      "Wall time: 8.49 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "results = []\n",
    "for va1 in range(len(Combinations))[:3]:\n",
    "    print('    process var '+str(va1)+' of '+str(len(Combinations)))\n",
    "    XWT_output=dask.delayed(XWT)(DailyVarsTrain[:,:,:,Combinations[va1]],\n",
    "                   DailyVarsEval[:,:,:,Combinations[va1]],\n",
    "                   rgrPRrecords,\n",
    "                   rgrPReval,\n",
    "                   TimeTrain,\n",
    "                   TimeEval,\n",
    "                   rgrNrOfExtremes[ne],\n",
    "                   SpatialSmoothing[sm],\n",
    "                   ClusterMeth='hdbscan')\n",
    "    results.append(XWT_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 9 s, sys: 3.74 s, total: 12.7 s\n",
      "Wall time: 23.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "XWT_output = dask.compute(*results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = dask.optimize(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    process var 0 of 6017\n",
      "CPU times: user 2.48 s, sys: 951 ms, total: 3.43 s\n",
      "Wall time: 3.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for va1 in range(len(Combinations))[:1]:\n",
    "    print('    process var '+str(va1)+' of '+str(len(Combinations)))\n",
    "    XWT_output=XWT(DailyVarsTrain[:,:,:,Combinations[va1]],\n",
    "                   DailyVarsEval[:,:,:,Combinations[va1]],\n",
    "                   rgrPRrecords,\n",
    "                   rgrPReval,\n",
    "                   TimeTrain,\n",
    "                   TimeEval,\n",
    "                   rgrNrOfExtremes[ne],\n",
    "                   SpatialSmoothing[sm],\n",
    "                   ClusterMeth='hdbscan')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'grClustersFin': (array([[ 1.2556458 ,  2.47910029,  4.60599936, ...,  1.00021202,\n",
       "          -1.90511102, -2.36943394]]),\n",
       "  array([0, 0, 0, 0])),\n",
       " 'grEucledianDist': array([ 970.94947101,  970.8806865 ,  970.86067471, ..., 1024.7699964 ,\n",
       "         956.16181874,  969.59714641]),\n",
       " 'EucledianDistAllWTs': array([[ 970.94947101],\n",
       "        [ 970.8806865 ],\n",
       "        [ 970.86067471],\n",
       "        ...,\n",
       "        [1024.7699964 ],\n",
       "        [ 956.16181874],\n",
       "        [ 969.59714641]]),\n",
       " 'grCorrelatio': array([-8.39356386e-02, -2.39383891e-02, -1.59223551e-16, ...,\n",
       "         8.96943211e-02,  6.14423914e-02, -2.73932990e-16]),\n",
       " 'grCorrelatioAllWTs': array([[-8.39356386e-02],\n",
       "        [-2.39383891e-02],\n",
       "        [-1.59223551e-16],\n",
       "        ...,\n",
       "        [ 8.96943211e-02],\n",
       "        [ 6.14423914e-02],\n",
       "        [-2.73932990e-16]]),\n",
       " 'grPSS': 0.08328767123287673,\n",
       " 'grMRD': 0.9925291538238525,\n",
       " 'grMRR': 1.0863723608445297,\n",
       " 'APR': 0.0012891610439048637,\n",
       " 'AUC': 0.45681381957773515,\n",
       " 'PEX': 14.050944946589972,\n",
       " 'grExluded': 14.050944946589972,\n",
       " 'PRanom': None,\n",
       " 'InterVSIntra': None}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "XWT_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
